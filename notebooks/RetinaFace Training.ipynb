{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81672af1",
   "metadata": {},
   "source": [
    "# The classifier of people with glasses \n",
    "\n",
    "The task is: implement a classifier for images with a human face, separating them into people with glasses and everyone else. It is assumed that the approach will be developed with an emphasis on its further use in a `mobile application in real-time`.\n",
    "\n",
    "Limitations:\n",
    "- common dependencies (opencv, dlib, numpy, scipy, tensorflow, ...) can be used, but they must be included in the installation instructions\n",
    "- any pre-trained models or ready-made algorithms can be used\n",
    "- any publicly available datasets for training and testing are allowed\n",
    "- languages: C++, Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5592e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19507ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58547a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('tkagg')\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad0a902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627b49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b0b17",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe9496",
   "metadata": {},
   "source": [
    "A sample set of 40 images has been provided by the team for testing (20 each for with/without glasses). In addition I have considered 3 more datasets for this project. Details of all datasets are mentioned below: \n",
    "\n",
    "1. Sample Test Set:\n",
    "    - with_glasses = 20 jpeg images\n",
    "    - without_glasses = 20 jpeg images\n",
    "\n",
    "<br> \n",
    "\n",
    "2. SoF\n",
    "    - Dataset page: https://sites.google.com/view/sof-dataset\n",
    "    - Images download link: https://drive.google.com/file/d/1ufydwhMYtOhxgQuHs9SjERnkX0fXxorO/\n",
    "    - Metadata download link: https://drive.google.com/file/d/0BwO0RMrZJCioaTVURnZoZG5jUVE/view?usp=sharing&resourcekey=0-F8-ejyF8NX4GC129ustqLg \n",
    "\n",
    "<br> \n",
    "\n",
    "3. Facial Landmark Detection by Deep Multi-task Learning:\n",
    "    - Dataset Page: http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html\n",
    "    - Images download link: http://mmlab.ie.cuhk.edu.hk/projects/TCDCN/data/MTFL.zip\n",
    "\n",
    "<br> \n",
    "\n",
    "4. MeGlass:\n",
    "    - Dataset Page: https://github.com/cleardusk/MeGlass/tree/master\n",
    "    - Images download link: https://drive.google.com/file/d/1V0c8p6MOlSFY5R-Hu9LxYZYLXd8B8j9q/view?usp=sharing\n",
    "    - Metadata download link: https://github.com/cleardusk/MeGlass/blob/master/meta.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed532895",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MTFL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96f448e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76054e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mtfl_dataset = Path(\"./data/MTFL\")\n",
    "mtfl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9b60d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(mtfl_dataset.joinpath('training.txt'), \n",
    "                    sep=' ',\n",
    "                    header=None,\n",
    "                    skipinitialspace = True,\n",
    "                    names=['Path']+['x1','x2','x3','x4','x5','y1','y2','y3','y4','y5']+['Gender','Smile','Glasses','Pose'])\n",
    "train['Path'] = train['Path'].str.replace('\\\\','/')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7766f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "--x1...x5,y1...y5: the locations for left eye, right eye, nose, left mouth corner, right mouth corner.\n",
    "--gender: 1 for male, 2 for female\n",
    "--smile: 1 for smiling, 2 for not smiling\n",
    "--glasses: 1 for wearing glasses, 2 for not wearing glasses.\n",
    "--head pose: 1 for left profile, 2 for left, 3 for frontal, 4 for right, 5 for right profile\n",
    "'''\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0932bcfe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mtfl_glasses_map = {1:'With Glasses',2:'No Glasses'}\n",
    "mtfl_pose_map = {1: '-60',2:'-30',3:'0',4:'+30',5:'+60'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc94db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_glass_images = train.loc[train['Glasses'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f8e9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualizing a random image with keypoints\n",
    "n = np.random.randint(low=2,high=1000)\n",
    "row = all_glass_images.iloc[n]\n",
    "\n",
    "img0 =  cv2.imread(str(mtfl_dataset.joinpath(row['Path'])))\n",
    "img0 = cv2.cvtColor(img0,cv2.COLOR_BGR2RGB)\n",
    "print(img0.shape)\n",
    "\n",
    "plt.scatter(row['x1'],row['y1'],c='r')\n",
    "plt.scatter(row['x2'],row['y2'],c='b')\n",
    "plt.scatter(row['x3'],row['y3'],c='y')\n",
    "plt.scatter(row['x4'],row['y4'],c='g')\n",
    "plt.scatter(row['x5'],row['y5'],c='w')\n",
    "plt.imshow(img0)\n",
    "plt.title(f\"{mtfl_glasses_map[int(row['Glasses'])]} Pose:{mtfl_pose_map[int(row['Pose'])]} deg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d44951",
   "metadata": {},
   "source": [
    "# Approach-2: Finetune Retinaface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c540b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"Pytorch_Retinaface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483bf75",
   "metadata": {},
   "source": [
    "## Step-1: MTFL Loader for Retinaface Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26edd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtfl_transforms = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MTFLDataset(annotations_file='./data/MTFL/training.txt',\n",
    "                             img_dir='./data/MTFL',\n",
    "                             transform=mtfl_transforms)\n",
    "\n",
    "val_dataset = MTFLDataset(annotations_file='./data/MTFL/testing.txt',\n",
    "                           img_dir='./data/MTFL',\n",
    "                           transform=mtfl_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def imshow(img, title):\n",
    "#     np_img = img.numpy().transpose((1, 2, 0))\n",
    "#     plt.imshow(np_img)\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "\n",
    "# # Iterate through the data loader and display the first 10 images along with their labels\n",
    "# num_images = 10\n",
    "# for i, (images, labels) in enumerate(train_loader):\n",
    "#     if i >= num_images:\n",
    "#         break\n",
    "#     imshow(images[0], f'Label: {labels.item()} (Glasses)' if labels.item() == 1 else f'Label: {labels.item()} (No glasses)')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFLDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(annotations_file, header=None, delim_whitespace=True, skipinitialspace=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(f\"self.img_dir: {self.img_dir}\")\n",
    "#         print(f\"annotations file: {self.annotations.iloc[idx, 0]}\")\n",
    "        img_path = os.path.join(self.img_dir, *self.annotations.iloc[idx, 0].split(\"\\\\\"))\n",
    "        image = Image.open(img_path)\n",
    "        glasses = self.annotations.iloc[idx, -2]\n",
    "        label = torch.tensor(1 if glasses == 1 else 0, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02890a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFLDataset(data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(annotations_file, header=None, delim_whitespace=True, skipinitialspace=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, *self.annotations.iloc[idx, 0].split(\"\\\\\"))\n",
    "        img = cv2.imread(img_path)\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        annotations = np.zeros((0, 15))\n",
    "        for idx, label in enumerate(labels):\n",
    "            annotation = np.zeros((1, 15))\n",
    "            # bbox\n",
    "            annotation[0, 0] = self.annotations.iloc[idx, 1] #label[0]  # x1\n",
    "            annotation[0, 1] = self.annotations.iloc[idx, 6]  # y1\n",
    "            annotation[0, 2] = self.annotations.iloc[idx, 2] #label[0] + label[2]  # x2\n",
    "            annotation[0, 3] = self.annotations.iloc[idx, 7] #label[1] + label[3]  # y2\n",
    "\n",
    "            # landmarks\n",
    "            annotation[0, 4] = self.annotations.iloc[idx, 1]    # l0_x\n",
    "            annotation[0, 5] = self.annotations.iloc[idx, 2]    # l0_y\n",
    "            annotation[0, 6] = self.annotations.iloc[idx, 3]    # l1_x\n",
    "            annotation[0, 7] = self.annotations.iloc[idx, 4]    # l1_y\n",
    "            annotation[0, 8] = self.annotations.iloc[idx, 5]   # l2_x\n",
    "            annotation[0, 9] = self.annotations.iloc[idx, 6]   # l2_y\n",
    "            annotation[0, 10] = self.annotations.iloc[idx, 7]  # l3_x\n",
    "            annotation[0, 11] = self.annotations.iloc[idx, 8]  # l3_y\n",
    "            annotation[0, 12] = self.annotations.iloc[idx, 9]  # l4_x\n",
    "            annotation[0, 13] = self.annotations.iloc[idx, 10]  # l4_y\n",
    "            \n",
    "            if (self.annotations.iloc[idx, -2]==1):\n",
    "                annotation[0, 14] = 1\n",
    "            else:\n",
    "                annotation[0, 14] = -1\n",
    "\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "            \n",
    "        target = np.array(annotations)\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "\n",
    "        return torch.from_numpy(img), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596a3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = np.zeros((1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc35cc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ce62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8561cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4f4e4f",
   "metadata": {},
   "source": [
    "## Step-2: Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdb35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.retinaface import RetinaFace\n",
    "from data import WiderFaceDetection, detection_collate, preproc, cfg_mnet, cfg_re50\n",
    "# from utils import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retinaface_weights = \"./weights/mobilenet0.25_Final.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ca8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the output channels in the model\n",
    "num_classes = 2  # Glasses or no glasses\n",
    "cfg_mnet['num_classes'] = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f88a0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# net and model\n",
    "net = RetinaFace(cfg=cfg_mnet, phase = 'test')\n",
    "net = load_model(net, retinaface_weights, True)\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13122e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification heads\n",
    "for param in net.ClassHead.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01967a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the loss function\n",
    "classification_loss = BCEWithLogitsLoss()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _ = net(images)\n",
    "        logits = logits.view(logits.size(0), num_classes, -1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = classification_loss(logits, labels.view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a4b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f268f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_folder = \"./data/sample_test/\"\n",
    "all_files = []\n",
    "for category in glob.glob(testset_folder+\"/*\"):\n",
    "    for file in glob.glob(category+\"/*\"):\n",
    "        all_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d394daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.jpg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(all_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f400c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/sample_test/with_glasses', './data/sample_test/without_glasses']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(testset_folder+\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957370a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
